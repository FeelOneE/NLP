{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################### 댓글 크롤링 시작 ############################\n",
    "######################################################################\n",
    "\n",
    "import urllib.parse as rep\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "#from hanspell import spell_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-judgment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Header 정보 초기화\n",
    "opener = req.build_opener()\n",
    "# User-Agent 정보\n",
    "#opener.addheaders = [('User-agent', UserAgent())]\n",
    "# Header 정보 삽입\n",
    "req.install_opener(opener)\n",
    "\n",
    "# 쇼핑 댓글 요청 헤더\n",
    "# https://search.shopping.naver.com/catalog/24178126692?cat_id=50000814&nv_mid=24178126692&NaPm=ct%3Dkmochwcg%7Cci%3D5fbcf90319f44a9577a7ba53d9afec4cf0af44e8%7Ctr%3Dsl%7Csn%3D95694%7Chk%3D180c476dc3ec1764ae9d356ee403773d5f2382ca\n",
    "# https://search.shopping.naver.com/catalog/14742158672?NaPm=ct%3Dkmyyltb4%7Cci%3D8fb2739c338be12299472ccff160ded9bc19122f%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D0186b9292ab060733115318dcfd65d3aefb527b7\n",
    "\n",
    "    \n",
    "# Request\n",
    "flag = True\n",
    "pageNum = 1\n",
    "result = []\n",
    "count = 0\n",
    "while flag:\n",
    "    \n",
    "    print(len(result))\n",
    "    # Throttle 설정\n",
    "    time.sleep(0.7)\n",
    "    try:\n",
    "        \n",
    "        # 쇼핑 댓글 요청 헤더\n",
    "        hdr = {'authority': 'search.shopping.naver.com', \\\n",
    "        'method': 'GET',\\\n",
    "        'path': '/review?nvMid=14742158672&reviewType=ALL&sort=QUALITY&isNeedAggregation=N&isApplyFilter=N&page='+str(pageNum)+'&pageSize=20',\\\n",
    "        'scheme': 'https',\\\n",
    "        'accept': 'application/json, text/plain, */*',\\\n",
    "        'accept-encoding': 'gzip, deflate, br',\\\n",
    "        'accept-language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\\\n",
    "        'cache-control': 'no-cache',\\\n",
    "        'pragma': 'no-cache',\\\n",
    "        'referer': 'https://search.shopping.naver.com/catalog/14742158672?NaPm=ct%3Dkmyyltb4%7Cci%3D8fb2739c338be12299472ccff160ded9bc19122f%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D0186b9292ab060733115318dcfd65d3aefb527b7',\\\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"89\", \"Chromium\";v=\"89\", \";Not A Brand\";v=\"99\"',\\\n",
    "        'sec-ch-ua-mobile': '?0',\\\n",
    "        'sec-fetch-dest': 'empty',\\\n",
    "        'sec-fetch-mode': 'cors',\\\n",
    "        'sec-fetch-site': 'same-origin',\\\n",
    "        'urlprefix': '/api',\\\n",
    "        'user-agent': 'M'}\n",
    "\n",
    "        url = 'https://search.shopping.naver.com/review?nvMid=14742158672&reviewType=ALL&sort=QUALITY&isNeedAggregation=N&isApplyFilter=N&page='+str(pageNum)+'&pageSize=20'        \n",
    "        \n",
    "        reqeust = req.Request(url,headers=hdr)\n",
    "        res = gzip.decompress(req.urlopen(reqeust).read()).decode('utf-8')\n",
    "        \n",
    "        data = json.loads(res)\n",
    "        result.extend(data['reviews'])\n",
    "        pageNum = pageNum + 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        # Too many request (429) 일 때 sleep 후 다시 요청\n",
    "        if e.code == 429:\n",
    "            print(\"429 error is occured\")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(str(e))\n",
    "            print('더 없음')\n",
    "            flag = False\n",
    "\n",
    "\n",
    "#res = req.urlopen(reqeust).read().decode('utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "####################################################################################\n",
    "######################## 저장된 크롤링 데이터 불러오기 ###############################\n",
    "####################################################################################\n",
    "\n",
    "#df = pd.DataFrame(result)\n",
    "df = pd.read_csv('review_data.csv')\n",
    "#df.to_csv('review_data.csv',sep=',', na_rep='NaN')\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################### 댓글 크롤링 끝 ##############################\n",
    "######################################################################\n",
    "\n",
    "print(df.iloc[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################### 댓글 TextRank ##############################\n",
    "######################################################################\n",
    "#!pip install kobert-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석기 Komoran\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "sent = '쑥 들어간 제품들을 평소에 좋아해서 쑥 화장품으로 대부분 쓰려고 해요 아침 저녁으로 매일 매일 쓰기에 부담이 없고 좋습니다자연약쑥 진정 토너라 자극없이 쓰기에도 좋고 녹차 루이보스 캐모마일 쑥 페퍼민트 로즈마리 유칼리투스잎이 들어간 그린 허브 성분이라 피부 진정에도 도움이 되는거 같아요'\n",
    "\n",
    "words = komoran.pos(sent, join=True)\n",
    "print(words )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleasing 함수 \n",
    "import regex as re\n",
    "def cleasing(text):\n",
    "    repl =''\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 자음, 모음 제거\n",
    "    text = re.sub(pattern= pattern, repl=repl, string=text)\n",
    "    #pattern = '[^\\w\\s]' # 특수기호 제거\n",
    "    pattern = '[^가-히\\s]' # 특수기호 제거\n",
    "    text = re.sub(pattern= pattern, repl=repl, string=text)\n",
    "    pattern = '<[^>]*>' # html 제거\n",
    "    text = re.sub(pattern = pattern, repl='',string=text)\n",
    "    return text\n",
    "df['length'] = df.content.str.len()\n",
    "df['clean_content'] = df['content'].map(lambda x: cleasing(x))\n",
    "df2 = df[ df.length > 30 ] \n",
    "df2 = df2['clean_content']\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hanspell import spell_checker\n",
    "\n",
    "check_result = []\n",
    "index = 0\n",
    "for i in df2:\n",
    "    try:\n",
    "        check_result.append(spell_checker.check(i).checked)\n",
    "\n",
    "        index = index+1\n",
    "    except:\n",
    "        print(\"error occur!\")\n",
    "        print(i)\n",
    "        continue\n",
    "\n",
    "check_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 교정한 데이터 저장\n",
    "\n",
    "#pd.DataFrame(check_result).to_csv('review_data_clean.csv',sep=',', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('review_data_clean.csv', header=None)\n",
    "df2= list(df2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def komoran_tokenizer(sent):\n",
    "    try:\n",
    "        words = komoran.pos(sent, join=True)\n",
    "        #words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "        words = [w for w in words if ('/NN' in w )]\n",
    "    except:\n",
    "        return [0]\n",
    "    return words\n",
    "\n",
    "total_analysis = []\n",
    "\n",
    "for row in df2:\n",
    "    try:    \n",
    "        token_list = komoran_tokenizer(row)\n",
    "        total_analysis.extend(token_list)\n",
    "    except:\n",
    "        #pass\n",
    "        continue\n",
    "\n",
    "print(total_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textranks.textrank import KeywordSummarizer\n",
    "\n",
    "keyword_summarizer = KeywordSummarizer(tokenize=komoran_tokenizer, min_count=2, min_cooccurrence=1) \n",
    "\n",
    "keyword_extract = keyword_summarizer.summarize(df2, topk=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noun = pd.DataFrame(keyword_extract, columns=['words','feq'])\n",
    "word_sep =  list(df_noun.words.str.split('/'))\n",
    "print(word_sep[0][1])\n",
    "df_noun['word'] = [ word_sep[i][0] for i in range(len(word_sep)) ]\n",
    "tmp_list = []\n",
    "for i in range(len(word_sep)):\n",
    "    item = word_sep[i][0]\n",
    "    tmp_list.append(item)\n",
    "\n",
    "df_noun['word'] = tmp_list\n",
    "df_noun = df_noun.drop(['words'], axis=1)\n",
    "df_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### 카테고리 정하기 #############\n",
    "\n",
    "# 명사만 추출한 것\n",
    "from collections import Counter\n",
    "\n",
    "count_result = Counter(total_analysis).most_common(300)\n",
    "print(count_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 카운팅 결과 DataFrame 으로 만들기\n",
    "df_noun = pd.DataFrame(count_result, columns=['words','feq'])\n",
    "\n",
    "word_sep =  list(df_noun.words.str.split('/'))\n",
    "print(word_sep[0][1])\n",
    "df_noun['word'] = [ word_sep[i][0] for i in range(len(word_sep)) ]\n",
    "tmp_list = []\n",
    "for i in range(len(word_sep)):\n",
    "    item = word_sep[i][0]\n",
    "    tmp_list.append(item)\n",
    "\n",
    "df_noun['word'] = tmp_list\n",
    "df_noun = df_noun.drop(['words'], axis=1)\n",
    "df_noun[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer 모델 로드하기\n",
    "\n",
    "import torch\n",
    "from kobert_transformers import get_kobert_model, get_distilkobert_model\n",
    "from transformers import BertModel, DistilBertModel\n",
    "\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "\n",
    "#print(model.embeddings.word_embeddings[0])\n",
    "#model = get_kobert_model()\n",
    "#model.eval()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavail_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pororo'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a84d34709178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#============ Pororo ====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpororo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPororo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPororo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ko\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"사용\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pororo'"
     ]
    }
   ],
   "source": [
    "#============ Pororo ====================\n",
    "from pororo import Pororo\n",
    "cse = Pororo(task=\"cse\", lang=\"ko\")\n",
    "cse(\"사용\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 임베딩 치환 테스트\n",
    "from kobert_transformers import get_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])\n",
    "input_ids2 = tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])\n",
    "\n",
    "input_ids2 = tokenizer.convert_tokens_to_ids(['한국'])\n",
    "input_ids3 = tokenizer.convert_tokens_to_ids(['모델'])\n",
    "input_ids4 = tokenizer.convert_tokens_to_ids(['구매'])\n",
    "input_ids5 = tokenizer.convert_tokens_to_ids(['한국'])\n",
    "\n",
    "\n",
    "#input_ids = torch.LongTensor([input_ids,input_ids2])\n",
    "input_ids = torch.LongTensor([input_ids2,input_ids3,input_ids4,input_ids5])\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "#attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "#token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "#last_hidden_state, pooled_output  = model(input_ids, attention_mask, token_type_ids, return_dict=False)\n",
    "last_hidden_state, pooled_output  = model(input_ids, return_dict=False)\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "#print(last_hidden_state[0])\n",
    "print(last_hidden_state.detach().numpy())\n",
    "last_hidden_state = last_hidden_state.detach().numpy()\n",
    "print('--'*20)\n",
    "result = 1 - spatial.distance.cosine(last_hidden_state[0], last_hidden_state[2])\n",
    "print(result)\n",
    "#print(cosine_similarity(last_hidden_state[0],last_hidden_state[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추출한 명사의 임베딩을 추출함\n",
    "\n",
    "words_list = [[] for _ in range(len(df_noun.word))]\n",
    "print(type(df_noun.word))\n",
    "\n",
    "for i,t in df_noun.word.iteritems():\n",
    "    tmp_list = []\n",
    "    tmp_list.append(t)\n",
    "    words_list[i] = tmp_list\n",
    "#print(words_list)\n",
    "\n",
    "## 단어가 있는 임베딩만 치환함\n",
    "index = 0\n",
    "words_list_not_zero = []\n",
    "check_word = []\n",
    "unavail_word = []\n",
    "for i in words_list:\n",
    "    #print(tokenizer.convert_tokens_to_ids(i),\" : \",df_noun.word[index])\n",
    "    if tokenizer.convert_tokens_to_ids(i)[0] != 0 :\n",
    "        words_list_not_zero.append(tokenizer.convert_tokens_to_ids(i))\n",
    "        check_word.append(df_noun.word[index])\n",
    "    else:\n",
    "        print(df_noun.word[index],\" is not in vocab\")\n",
    "        unavail_word.append(df_noun.word[index])\n",
    "    index = index +1 \n",
    "\n",
    "print(check_word)\n",
    "print(len(check_word))\n",
    "\n",
    "input_ids = torch.LongTensor(words_list_not_zero)\n",
    "last_hidden_state, pooled_output  = model(input_ids, return_dict=False)\n",
    "\n",
    "sentence_glove = torch.flatten(last_hidden_state, start_dim=1).detach().numpy()\n",
    "print(sentence_glove.shape)\n",
    "print(sentence_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# clustering\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=2021)\n",
    "y_pred = kmeans.fit_predict(sentence_glove)\n",
    "\n",
    "\n",
    "# tsne\n",
    "tsne = TSNE(verbose=1, perplexity=100, random_state=2021)     # perplexity : 유사정도\n",
    "z = tsne.fit_transform(sentence_glove)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = y_pred\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "plt.scatter(z[:,0],z[:,1], c=y_pred, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "result = dict()\n",
    "for i in y_pred:\n",
    "    #print(i, \" : \",check_word[index] )\n",
    "    if i in result:\n",
    "        result[i].append(check_word[index])\n",
    "    else:\n",
    "        result[i] = [check_word[index]]\n",
    "    index = index + 1\n",
    "for i in result:\n",
    "    print(result[i])\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank 중요문장 10개 추출\n",
    "from textranks.textrank import KeysentenceSummarizer\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "\n",
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = komoran_tokenizer,\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "\n",
    "keysents = summarizer.summarize(list(df2)[:], topk=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in keysents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pororo'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-3152e442e469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpororo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPororo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPororo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abstractive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ko\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pororo'"
     ]
    }
   ],
   "source": [
    "from pororo import Pororo\n",
    "summ = Pororo(task=\"summarization\", model=\"abstractive\", lang=\"ko\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}